#+title: 離散隠れ空間での記号推論
#+include: "head.org"
#+LINK: img file:img/%s
#+LINK: png file:img/%s.png
#+LINK: jpg file:img/%s.jpg
#+LINK: svg file:img/%s.svg
#+LINK: spng file:img/static/%s.png
#+LINK: sjpg file:img/static/%s.jpg
#+LINK: ssvg file:img/static/%s.svg
#+LINK: sgif file:img/static/%s.gif

#+begin_outline-text-1
#+begin_center

#+begin_larger
Masataro Asai

The University of Tokyo
#+end_larger

20+ min
#+end_center

#+begin_note
#+begin_alignright
Made by guicho2.71828 (Masataro Asai)
#+end_alignright
#+end_note
#+end_outline-text-1

* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*AMA_2 Overview*

*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* Overview

#+begin_center
#+begin_xlarge
*/Backgrounds/*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*AMA_2 Overview*

*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* 背景 -- AIプランニング

 #+begin_container-fluid
 #+begin_row-fluid
 #+begin_span6
 [[png:astro/1]]
 #+end_span6
 #+begin_span6
 [[png:rescue/1]]
 #+end_span6
 #+end_row-fluid
 #+end_container-fluid

** 誰?

 #+begin_container-fluid
 #+begin_row-fluid
 #+begin_span6
 [[png:astro/1]]
 #+end_span6
 #+begin_span6
 [[png:rescue/1]]
 #+end_span6
 #+end_row-fluid
 #+end_container-fluid

  #+begin_resume
  and let me introduce these robots.
  the guy in the left is astro boy.
  #+end_resume

*** 誰?

 #+begin_container-fluid
 #+begin_row-fluid
 #+begin_span6
 [[png:astro/2]]
 #+end_span6
 #+begin_span6
 [[png:rescue/1]]
 #+end_span6
 #+end_row-fluid
 #+end_container-fluid

 #+begin_resume
 as you know, he is a famous manga superhero invented by tezuka osamu in 50s,
 #+end_resume

*** 誰?

  #+begin_container-fluid
  #+begin_row-fluid
  #+begin_span6
  [[png:astro/final]]
  #+end_span6
  #+begin_span6
  [[png:rescue/1]]
  #+end_span6
  #+end_row-fluid
  #+end_container-fluid

 #+begin_resume
 and he can think, hear, speak, act. he also has emotions.
 #+end_resume

*** 誰?

  #+begin_container-fluid
  #+begin_row-fluid
  #+begin_span6
  [[png:astro/final]]
  #+end_span6
  #+begin_span6
  [[png:rescue/2]]
  #+end_span6
  #+end_row-fluid
  #+end_container-fluid

 #+begin_resume
  in contrast, the guy in the right is a real robot that is actually in use @ fukuoka prefecture for the rescue purpose..
  his name is t-52 enryu, developped by a japanese company temzak.
  he is huge and powerful -- about 4 meters in height and can carry things which is as heavy as 500kg.
  well, so, in a sense, he is also a superhero in the real disastrous situation.
 #+end_resume

*** 誰?

  #+begin_container-fluid
  #+begin_row-fluid
  #+begin_span6
  [[png:astro/final]]
  #+end_span6
  #+begin_span6
  [[png:rescue/3]]
  #+end_span6
  #+end_row-fluid
  #+end_container-fluid

 #+begin_resume
 but does he have feelings or can he think? can he even move around by his own?
 #+end_resume

*** 誰?

  #+begin_container-fluid
  #+begin_row-fluid
  #+begin_span6
  [[png:astro/final]]
  #+end_span6
  #+begin_span6
  [[png:rescue/final]]
  #+end_span6
  #+end_row-fluid
  #+end_container-fluid

 #+begin_resume
 no. it requires full human intervention --- it is indeed operated by a
 driver who gets in or by a remote control. it is more like a
 super-sophisticated shovel car.
 #+end_resume

** 実際の大規模災害では非実用的 --- 操縦士が足りない!              :noexport:

 #+begin_container-fluid
 #+begin_row-fluid
 #+begin_span2
 [[png:rescue]]
 [[png:rescue]]
 [[png:rescue]]
 [[png:silent]]
 #+end_span2
 #+begin_span10
 [[jpg:static/tsunami]]
 #+end_span10
 #+end_row-fluid
 #+end_container-fluid

 #+begin_larger
 #+begin_alignright
 + そのままでは役に立たない!
 #+end_alignright
 #+end_larger

 #+begin_resume
 now the problem is : it's ok in small accidents but is impractical in the real, massive 
 natural disaster which frequently occurs in japan.
 the key resource is human ---
 these special purpose vehicles require human intervention,
 thus they are useless without trained operators.
 #+end_resume

*** 操縦士を増やせない -- human resource and training

 #+begin_container-fluid
 #+begin_row-fluid
 #+begin_span4
  [[png:rescue/1]]
 #+end_span4
 #+begin_span8

   + ✘ /時間/ がかかる :: 訓練に ＞100時間, *必要な時だけ増やす* のは不可能
   + ✘ /￥￥￥￥/ がかかる :: 訓練官、訓練場所、訓練用具
   + ✘ 技術は /維持が重要/ :: 定期的な再訓練、長期的コスト、さらなるマニー
   + ✘ 平時は /無駄/ な技術 :: 普段は意味がない -- 無駄なマニー!
 #+end_span8
 #+end_row-fluid
 #+end_container-fluid

 #+begin_resume
 in a natural disaster, we need as many experienced operators as possible.
 however, it is virtually impossible due to several reasons. 

 first, training takes time.
 it is impossible to quickly increase the number of operators as needed, at the time of disaster.

 second, the money matters.
 training a person costs a lot of money, including: the cost of maintaining
 a training center, the cost of additional vehicles for training, the cost
 of training the trainers, wages for trainers, etc.

 third, skills need to be updated and maintained.
 you know, how about preparing the large number of operators in advance?
 no, the society cannot torelate the cost of keep training them.
 operators may lose the skills and skills may become outdated.

 finally, in a normal situation, those skills are useless.
 it forces the society to waste a great amount of extra money.
 #+end_resume

** 自律行動のための自動プランナ (≠ モータ制御)

 [[png:planning/1]]

 #+begin_resume
 研究テーマのプランニングは、ロボットに、人間の助けを借りず、いかに自律して行動させるかを扱います。
 これをモデル化したプランニング問題は、具体的な行動の列を求める 組合せ最適化問題です。

 プランニング問題のタスクは、
 センサーから初期状態とゴールを受け取って、被災者を助ける正しい手順を出力することです。

 たとえば、この図では男性が瓦礫に埋まって助けを求めています。
 プランニング機能のあるロボットは、コレに対して「男性を助けよ」という大まかな指示を受けます。
 #+end_resume

** 自律行動のための自動プランナ (≠ モータ制御)

 [[png:planning/2]]

 #+begin_resume
 指示の内容には、図のように初期状態とゴール、許可された行動のリストが入っています。
 ロボットは、自動プランニングにより、人間の代わりに適切な行動を組み立てて、ゴールを自動で達成します。
 #+end_resume

** 自律行動のための自動プランナ (≠ モータ制御)

 [[png:planning/final]]

 #+begin_resume
 プランニングは汎用な枠組みなので、災害救助以外にも様々な問題に適用することができます。
 現実の応用例では「宇宙探査機運行問題」や「企業ネットワーク脆弱性問題」も表現できます。

 このように、プランニングは、難しい問題を汎用性を失わずに解くことを目指します。
 #+end_resume

** AIプランニングの */Killer App/*                                 :noexport:


#+BEGIN_CONTAINER-FLUID
#+BEGIN_ROW-FLUID
#+BEGIN_SPAN7
#+BEGIN_LARGER
+ 人が高価or不可能な作業 :: 原発, 宇宙空間, 火星, 深海
+ 正しさと最適性の理論保証が必要なミッションクリティカルシステム :: 
     製造システム、運送 (時間=お金)

     人工衛星 (燃料使いきれば運用終了)

     間違った解は許されない
+ 思考過程を説明可能なシステム :: 
     レスキュー・宇宙船 (人間の安全がかかっている)
#+END_LARGER

# [[sjpg:martian]]

#+END_SPAN7
#+BEGIN_SPAN5

[[sjpg:gravity-m]]

#+END_SPAN5
#+END_ROW-FLUID
#+END_CONTAINER-FLUID



** AIと自動プランニング の位置づけ -- /理論/ と /実応用/ の中間    :noexport:

 緑は /理論/ 、オレンジは /実応用/ 、 AI はその橋渡し (どれともかぶらない部分もある)

 #+BEGIN_RESUME
 Automated Planning is a branch of Aritificial Intelligence. 

 It shares a lot of technology with Operations Research and Theoretical
 Computer Science, and is considered a bridge between pure theory and
 pure applications.
 #+END_RESUME

 [[png:planning-related-field]]

* 古典プランニング問題 (決定的,完全情報) -- Blocksworld            :noexport:

#+HTML: <embed src="img/plan.svg" type="image/svg+xml"  />

#+begin_larger
非古典的なさまざまな拡張
#+begin_alignright
(並列アクション,POMDP,HTN... AIの教科書を参照)
#+end_alignright
#+end_larger

** アクション = 条件付き状態遷移

#+begin_center
#+begin_xlarge
アクション (move ?x ?y)
#+end_xlarge
#+end_center

#+BEGIN_CENTER
*?X*, *?Y* : 変数。 値 *BLOCK-A*, *BLOCK-B* などを適用して使う

*条件* と *効果* で構成される
#+END_CENTER


#+begin_container-fluid
#+begin_row-fluid
#+begin_span7
#+begin_quote
*条件* : 実行に必要な条件を表す命題

　(clear *?X*) : 積み木 *?X* の上が空

　(clear *?Y*) : 積み木 *?Y* の上に空

*効果* : 前後の状態の *差分* を表す命題

　(on *?X* *?Y*) を *追加* : *?Y* の上は *?X*

　(clear *?Y*) を *削除*
#+end_quote
#+end_span7
#+begin_span5
#+begin_src lisp
(:action move
 :parameters (?X ?Y)
 :preconditions
   (and (clear ?X)   ; (1)
        (clear ?Y))  ; (2)

 :effect
   (and (on ?X ?Y)   ; (3)
        (not         ; (4)
         (clear ?Y))))
#+end_src
#+begin_alignright
#+begin_larger
モデリング言語 *PDDL* で記述
#+end_larger
#+end_alignright
#+end_span5
#+end_row-fluid
#+end_container-fluid

** *PDDL* : Planning Domain Description Language                   :noexport:

International Planning Competition で使われている入力形式

#+BEGIN_CONTAINER-FLUID
#+BEGIN_ROW-FLUID
#+BEGIN_SPAN2

#+END_SPAN2
#+BEGIN_SPAN8
#+BEGIN_SRC lisp
(:action move
 :parameters (?X ?Y)
 :preconditions
   (and (clear ?X)   ; (1)
        (clear ?Y))  ; (2)

 :effect
   (and (on ?X ?Y)   ; (3)
        (not         ; (4)
         (clear ?Y))))
#+END_SRC
#+END_SPAN8
#+BEGIN_SPAN2

#+END_SPAN2
#+END_ROW-FLUID
#+END_CONTAINER-FLUID

** プランニング = グラフ探索

*ノード* : 状態 = 命題の集合 ⇒ =(on A B)=, =(clear A)= など

*辺*     : アクション ⇒ =(move A B)= 等

[[png:graph]]

# #+BEGIN_CONTAINER-FLUID
# #+BEGIN_ROW-FLUID
# #+BEGIN_SPAN6
# # + ヒューリスティック探索 A*
# # + State-of-the-Art *1
# #+END_SPAN6
# #+BEGIN_SPAN6
# # #+attr_html: :width 50%
# #+END_SPAN6
# #+END_ROW-FLUID
# #+END_CONTAINER-FLUID

#+BEGIN_NOTE
*1 [Helmert, 2006] [Richter, 2010]
#+END_NOTE
  
** Q. いま */はやり/* のDeep Learningとの違いは?                   :noexport:

 A. レイヤが違う

 #+BEGIN_CONTAINER-FLUID
 #+BEGIN_ROW-FLUID
 #+BEGIN_SPAN6
 *機械学習・Neural Networks* 
 
 for *認識・反射*
 + 入力 は *Subsymbolic* (連続値)
   
   画像、音声、非構造化テキスト: 
 + *感覚的知能*:
   
   　 */反応/, /直後/ の行動の決定*
   #+BEGIN_SMALLER
   *パブロフの犬* : 餌を認知→よだれ

   *自動運転* : 赤信号,人 → 止まる.

   *翻訳* : 文章 → 文章

   *囲碁局面の評価関数* : 局面 → 勝率
   #+END_SMALLER
   #+BEGIN_LARGER
   ☺ 効率よく 1-to-1 mapping
   
   ☹ 単純作業
   #+END_LARGER
 #+END_SPAN6
 #+BEGIN_SPAN6
 *推論・探索*

 for *プランニング・ゲーム・定理証明*
 + 入出力は *Symbolic*
   
   論理 オブジェクト ルール
 + *論理・推論による知能:*

   　 */未来に渡る/ 戦略の決定*
   
   　 (戦略 = 行動の *列や木*)
   #+BEGIN_SMALLER
   *レスキューロボ* : ゴール = 被災者生存

   *証明器* : ゴール = QED

   *コンパイラ* : 命令列の生成
   
   *囲碁,将棋* : ゴール = 勝利
   #+END_SMALLER
   #+BEGIN_LARGER
   ☺ 順序制約+複雑な作業
   #+END_LARGER
 #+END_SPAN6
 #+END_ROW-FLUID
 #+END_CONTAINER-FLUID

+ AlphaGo = Subsymbolic (DLNNによる評価関数) + Symbolic (MCTSによる探索)

** 既存の有名システム                                              :noexport:

AlphaGo = Subsymbolic (NNによる評価関数) + Symbolic (MCTSによる探索)
+ ただし *ドメイン依存* -- 囲碁に特化, "マス目"や"石"といった概念をハードコード
+ *膨大な棋譜が必要* --- 運用データがない環境(e.g.火星)には適用不能
+ */人って模範解答がないと行動できませんか?/* *真の自律機械は前例無しでも行動可能*

DQN = Subsymbolic (DLNN) + 強化学習 (DLNN)

様々な Atari Game につかえる汎用フレームワーク (Invader, Packman…) だが
+ RLのActing: 学習したpolicyに従ってgreedyに行動
+ Atariゲームは *脊髄反射で生き残ることが可能* → *複雑な論理思考はいらない!*
  
# 実際 *Sokoban など論理思考ゲームでは性能が悪い* ↔ 倉庫番ソルバ



* スライディングタイルパズル (a.k.a 8-puzzle)

#+begin_container-fluid
#+begin_center
#+begin_row-fluid
#+begin_span4
[[png:8puzzle-standard]]
Initial State
#+end_span4
#+begin_span4
[[png:8puzzle-standard-goal]]
Goal State
#+end_span4
#+begin_span4
[[sgif:8puzzle]]
#+end_span4
#+end_row-fluid
#+end_center
#+end_container-fluid

#+begin_alignright
+ *ゴールに到達するアクション列=プランを探すのが目的*
#+end_alignright

** 状態

*状態* は *一階述語論理の命題列* , モデリング言語 *PDDL* で表される.

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
#+begin_quote
#+begin_smaller
/Empty(x_0, y_0)/

/Is(panel_6, x_1, y_0)/

/Up(y_0, y_1), Down(y_1, y_0).../

/Right(x_0, x_1), Left(x_0, x_1).../
#+end_smaller
#+end_quote
#+end_span6
#+begin_span6
#+begin_src lisp
(empty x0 y0)
(is panel6 x1 y0)
(up    y0 y1), (down y1 y0)...
(right x0 x1), (left x0 x1)...
#+end_src
#+end_span6
#+end_row-fluid
#+begin_center
#+begin_row-fluid
#+begin_span4
[[png:8puzzle-standard]]
Initial State
#+end_span4
#+begin_span4
[[png:8puzzle-standard-goal]]
Goal State
#+end_span4
#+begin_span4
[[sgif:8puzzle]]
#+end_span4
#+end_row-fluid
#+end_center
#+end_container-fluid

** 状態遷移 / アクション

#+begin_container-fluid
#+begin_row-fluid
#+begin_span9
*上にスライド* を表すルール → *アクション*

#+begin_quote
+ *上が空きなら* 上にスライド可能 (*前提条件*)

+ *効果*: 今の場所は *空きになり*

+ 上は *¬空き* になる.
#+end_quote
#+end_span9
#+begin_span3
initial state
[[png:8puzzle-standard-tile7]]
#+end_span3
#+end_row-fluid
#+begin_row-fluid
#+begin_span12
+ *一階述語論理*
  
  *When* /Empty(x, y_{old}) ∧ is(panel, x, y_{new}) ∧ up(y_{new}, y_{old})/ ;
  
  *then* /¬ Empty(x,y_{old}) ∧ Empty(x,y_{new}) ∧ ¬ is(panel, x, y_{new}) .../

+ *PDDL Model* : プランナへの実際の入力
  #+begin_src lisp
  (:action slide-up ...
   :precondition (and (empty ?x ?y-old) ...)
   :effects (and (not (empty ?x ?y-old)) (empty ?x ?y-new) ...))
  #+end_src

# #+begin_src lisp
# (:action move-up
#  :parameters (?x ?y-old ?y-new ?panel)
#  :precondition (and (empty ?x ?y-old)
#                     (up ?y-old ?y-new)
#                     (is ?panel ?x ?y-new))
#  :effects (and (not (empty ?x ?y-old))
#                (empty ?x ?y-new)
#                (not (is ?panel ?x ?y-new))
#                (is ?panel ?x ?y-old)))
# #+end_src
#+end_span12
#+end_row-fluid
#+end_container-fluid

# + 
#   #+begin_larger
#   #+begin_alignright
#   But *where does this representation come from?*
#   #+end_alignright
#   #+end_larger

** プランニング = グラフ探索

*ノード* : 状態 = 命題の集合 ⇒ =(on A B)=, =(clear A)= など

*辺*     : アクション ⇒ =(move A B)= 等

[[png:graph]]

# #+BEGIN_CONTAINER-FLUID
# #+BEGIN_ROW-FLUID
# #+BEGIN_SPAN6
# # + ヒューリスティック探索 A*
# # + State-of-the-Art *1
# #+END_SPAN6
# #+BEGIN_SPAN6
# # #+attr_html: :width 50%
# #+END_SPAN6
# #+END_ROW-FLUID
# #+END_CONTAINER-FLUID

#+BEGIN_NOTE
*1 [Helmert, 2006] [Richter, 2010]
#+END_NOTE
  
** 記号的プランナは */画像ベースの/* 8-puzzle を解けない

#+begin_center
+ *現在の計算機+現在のソルバは、8パズルを、PDDLさえあれば0.1秒以下で最適に解ける。*
#+end_center

#+begin_container-fluid
#+begin_row-fluid
#+begin_span8
[[sjpg:puzzle]]
#+end_span8
#+begin_span4
+ 
   #+begin_center
   *しかし*

   */PDDL/*

   */モデルが/*

   */無い!!!/*
#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

** 知識獲得のボトルネック (Cullen, 1988):

#+begin_quote
*現実の問題* を *記号的システム* に解かせるため、
*問題を人間が記号モデル化* するのにかかるコスト。
#+end_quote

#+begin_container-fluid
#+begin_row-fluid
#+begin_span12
+ *いまある画像版8パズルでは、２つの作業が必要*:
#+end_span12
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
+ *1. シンボルグラウンディング:*

  #+begin_center
  #+begin_larger
  */シンボル/ = 独立した要素*
  #+end_larger
  #+end_center
  
  シンボルには *種類* がある。
  #+begin_smaller
  | 種類         | 例                           |
  |--------------+------------------------------|
  | オブジェクト | *panel7*, *x_0*, *y_0* ...   |
  | 述語         | (*empty* ?x ?y)              |
  | 命題変数     | *p_28* = (empty x_0 y_0)     |
  | アクション   | (*slide-up* panel_7 x_0 y_1) |
  #+end_smaller
#+end_span6
#+begin_span6
+ *2. アクションモデルの獲得(AMA):*
  
  (Action Model Acquisition)

  #+begin_center
  #+begin_larger
  *環境を学習し、シンボルで説明する.*
  #+end_larger
  #+end_center
  
  　

  #+begin_center
  *When* /Empty(x, y_{old}) ∧ .../ ;

  *Then* /¬Empty(x,y_{old}) ∧/ ...
  #+end_center

#+begin_center
+ 
  #+begin_larger
  *この２つを同時に解決するシステムを提案します。*
  #+end_larger
#+end_center

#+end_span6
#+end_row-fluid
#+end_container-fluid

# #+begin_note
# The knowledge acquisition bottleneck: time for reassessment? : Cullen, J and Bryman, A Expert Syst. Vol 5 No 3 (August 1988) pp 216-225
# #+end_note

* Latent-Space Planner (*/Latplan/*)

#+begin_larger
 (*/Accepted/* in AAAI-2018)
#+end_larger

[[png:latplanlogo]]

* Survey of Exisiting Action Model Acquisition Techniques          :noexport:

#+begin_xlarge
Survey of Exisiting Action Model Acquisition Techniques
#+end_xlarge

　

#+begin_alignright
i.e. Systems that find action models
#+end_alignright

** Limitations of Existing Systems                                 :noexport:

#+begin_xlarge
#+begin_center
So far, ALL existing AMA systems require */symbolic / near-symbolic, accurate state inputs/* and/or */discrete action labels/*.
#+end_center
#+end_xlarge

#+begin_alignright
i.e. They need symbols to find an action model
#+end_alignright

** So far, ALL existing AMA systems require */symbolic inputs/*

ARMS (Yang AIJ07)
LOCM (ICAPS09)
Argall (AIJ09)
Mourao (UAI12)

All taking the *symbolic* inputs to find the *action models*

[[spng:locm]]

** Framer (ICAPS17)                                                :noexport:

*Near-Symbols* : Parses natural language sentences with a *clear grammatical structure*.


#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[spng:framer]]
#+end_span6
#+begin_span6
+ Alleviates the burden of domain experts, but *still requires human*
+ Not handling "Natural Language":
  
  #+begin_quote
  Pick up that parcel over there ... yeah, it has a label on it, it says Parcel1, you can see
  it from here, the Location B. Then put it in the car, I mean the truck, the red one.
  #+end_quote
#+end_span6
#+end_row-fluid
#+end_container-fluid

** Konidaris, Kaelbring (AAAI14, IJCAI15)                          :noexport:

"Constructing Symbolic Representations for High-Level Planning" (AAAI14)

+ What it does :: Converting a *Semi-MDP Model* to a *PDDL Model* by set-theoretic representation
                  
                  i.e. *Model-to-Model* conversion, not *generating a model from the scratch*
+ Semi-MDP contains Action Labels :: =move= and =interact= (Playroom)
+ Sensor inputs are structured (Labels for "State Variable" are known) :: 

     x/y-distance, light level, whether a monkey cries
     
     → Each sensor has a distinct meaning (no overwrap)

# + Low-dimensional, accurate input :: 33 vars (Playroom), 9 vars (Treasure), no noise
#      
#      Although IJCAI15 shows "visual depiction", it is not used by the system

** Learning from Video for Board Game (Bardu ICRA10; Kaiser AAAI12; Kirk 16) :noexport:

*Handles Images, but with strong assumptions (almost symbol)* e.g.

#+begin_quote
Tic-Tac-Toe with *Ellipse Detectors* (Bardu 10)
     
→ Almost immediately provides propositions

→ Also, Domain-dependent ("3x3 grid" "Ellipse" are hard-coded)
#+end_quote

* 問題設定

#+begin_center
#+begin_xlarge
Latplan の問題設定
#+end_xlarge
#+end_center

** 事前知識なし 画像入力のみで8パズルを解くプログラム

*事前知識なし* : 「9マスある」「動くパネル」など人の与えた *ラベル・シンボルなし*

[[sjpg:puzzle]]

** */事前知識なし/* 画像入力のみで8パズルを解くプログラム

*事前知識なし* : 「9マスある」「動くパネル」など人の与えた *ラベル・シンボルなし*

[[sjpg:puzzle]]

** 事前知識なし 画像入力のみで */あらゆるプランニング問題/* を解けるプログラム

*/事前知識なし/* : */画像ベースのドメイン非依存プランナ/*

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
Tower of Hanoi

[[sjpg:hanoi]]
#+end_span6
#+begin_span4
Lights-Out

[[sjpg:lightsout]]
#+end_span4
#+end_row-fluid
#+end_container-fluid

** システムの入力

#+begin_xlarge
２つの入力:

+ Training 入力
+ Planning 入力
#+end_xlarge

** 入力1: Training 入力 -- 画像ペアの集合

[[png:overview/1]]

** 入力1: Training 入力 -- 画像ペアの集合

#+begin_right
[[png:overview/2]]
#+end_right

環境から *ランダムに* 生成した遷移

+ *報酬なし* / *エキスパート知識なし*
+ *シミュレータなし* (Deep RLはシミュレータを常に使う)
+ 
  #+begin_larger
  */アクションラベルなし/*

  → ある画像の遷移で「何が起きているか」の説明がまったく与えられない
  #+end_larger

** 入力2: Planning 入力 -- 初期画像 & ゴール画像

[[png:overview/input2]]

** 事前知識なし 画像入力のみで */あらゆるプランニング問題/* を解けるプログラム

#+HTML: <embed src="img/overview/3.svg" type="image/svg+xml"  />

** 事前知識なし 画像入力のみで */あらゆるプランニング問題/* を解けるプログラム

#+HTML: <embed src="img/overview/3-hanoi.svg" type="image/svg+xml"  />

* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*/Latplan Architecture/*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*AMA_2 Overview*

*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* Latent-Space Planner (*/LatPlan/*) architechture

 [[png:overview/planning1]]

** Step 1: 命題変数シンボルグラウンディング

 [[png:overview/planning2]]

*** Step 1: State Autoencoder                                      :noexport:

[[png:train-state-ae]]

Trained SAE provides two functions:

+ $b = Encode(r)$ *maps a raw datum $r\;$ to a bit vector $b\;$*

+ $\tilde{r} = Decode(b)$ *maps a bit vector $b\;$ to a raw datum $\tilde{r}$*

** Step 2: アクションモデルの獲得 (AMA)

 [[png:overview/planning3]]

** Step 3: 記号的プランニング問題を記号的ソルバで解く

  [[png:overview/planning4]]

** Step 4: 記号的プランを実行し記号的状態列を得る

  [[png:overview/planning5]]

** Step 5: 記号的状態列を現実の写真に変換する

  [[png:overview/planning6]]

#+begin_center
#+begin_larger
#+end_larger
#+end_center

** まとめ

#+begin_xlarge
Latplan: 隠れ空間プランナ (Latent-space Planner).
#+end_xlarge

#+begin_larger
+ */現実の表現/ と /命題論理表現/ を教師無しで相互変換.*
+ */命題論理を用いて正しい推論を行う/.*
+ */現実の表現で答えを出力する/* (画像など).
+ */教師無しで完全自動で動作する/.*
#+end_larger

* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*/State AutoEncoder (SAE)/*
#+end_xlarge

break

#+begin_xlarge
*AMA_2 Overview*

*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* State AutoEncoder (SAE)

SAE はふたつの関数を含む *ニューラルネット*:

+ $b = Encode(r)$ : *生データ $r\;$ をビットベクタ $b\;$ に変換*

+ $\tilde{r} = Decode(b)$ : *ビットベクタ $b\;$ を生データ $\tilde{r}$ に逆変換*

#+begin_larger
+ *サブシンボリック表現とシンボリック表現の双方向マッピング*

  #+begin_alignright
   (ビットベクタ = 命題変数と同値)
  #+end_alignright
#+end_larger

** Neural Network 101                                              :noexport:

[[png:deeplearning/1]]

** Neural Network 101                                              :noexport:

[[png:deeplearning/2]]

** Neural Network 101                                              :noexport:

[[png:deeplearning/3]]

** Stochastic Gradient Descent + GPU                               :noexport:

[[spng:gradient-descent]]

Plus misc techniques e.g. *Batchnorm*, *Dropout*

#+begin_larger
*Pretty much everything is on the standard online tutorial / lecture cource / MOOP*

*Good libraries --- Tensorflow, Keras* --- you can learn in 1-2 months
#+end_larger

** 「ふつうの」分類タスク用のNN (教師あり)

# Target Function $y=f(x)\;$ trained by SGD minimizing $|y-f(x)|$
# 
# #+begin_alignright
# SGD: Stochastic Gradient Descent
# #+end_alignright
  
| Task           | Input x | Output y                     |
|----------------+---------+------------------------------|
| 画像クラス分け | 画像    | ラベル (1=🚗, 2=😺, 3=🐵 ...) |
# | Translation          | Sentence | Sentence                           |
# | Go eval. function    | State    | Number                             |

#+begin_larger
 + *これではLatplanには使えない*
   + *Latplan入力に人の与えた /ラベルはない/* 、つまり
   + シンボルを */どうグラウンディングするか/* は機械の気分次第
     
     　

     #+begin_alignright
     (*実際、人間も、同じものを別の形で記号接地しうる。*
     
     cf. 虹🌈 は何色で出来ている?)
     #+end_alignright

#+end_larger

** オートエンコーダ(AE)を使った教師なし学習

# Auto = "self" --- Autoencoding = "encoding itself"

#+begin_container-fluid
#+begin_row-fluid
#+begin_span7
　

目標関数: Identity $x=f(x)$
+ $x\;$ を *隠れ表現* $z$ にエンコード
+ $z\;$ を 入力 $x$ にデコード
+ $z\;$ は小さな次元数: 次元圧縮
+ 訓練: $|x - f(x)|\;$ を最小化
  (*自己符号化ロス*)
#+end_span7
#+begin_span5
[[png:deeplearning/autoenc]]
#+end_span5
#+end_row-fluid
#+end_container-fluid

#+begin_alignright
#+begin_larger
+ → しかし, */✘ 隠れベクタ Z は実数値/*

  */命題論理による推論には使えない/*
#+end_larger
#+end_alignright

** Variational AutoEncoder (VAE)                                   :noexport:

An AutoEncoder that *enforce a certain distribution* on $Z \subset \mathbb{R}^n$ over the dataset $X$

#+begin_quote
You have $X=$ { 10k images of apples }. If you train a *Gaussian VAE* on $X$, then $Z = Encode(X) \approx N(\mu,\sigma)$ for some $\mu,\sigma \in \mathbb{R}^n$.
#+end_quote

VAE needs a *reparametrization trick* because random distributions are non-differentiable.

#+begin_quote
Reparametrization for $N(\mu,\sigma)$: $\mu + \sigma N(0,1)$

#+begin_center
\mu and \sigma are differentiable vectors, $N(0,1)$ is not.
#+end_center
#+end_quote

** Gumbel-Softmax VAE (Jang, Gu, ICLR2017)

*AEにコスト関数を追加* し、 $Z \sim \textbf{Categorical}$ に収束するよう強制:

　　　→ $z\;$ は 1-hot ベクタに収束 e.g.  $\langle 0,0,1,0 \rangle$ .

例: MNIST 画像を 8カテゴリ変数 30個で表現

#+begin_center
 #+begin_html
 <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="649px" height="206px" version="1.1" content="&lt;mxfile userAgent=&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36&quot; version=&quot;6.0.1.2&quot; editor=&quot;www.draw.io&quot; type=&quot;google&quot;&gt;&lt;diagram name=&quot;Page-1&quot;&gt;3ZhLc5swEMc/Dcd0kAQCX+O67SGd6TTTaXqU0fJoZcsjy69++oogDBjs0MZ2TONDpL9ey29Xi5BDxrPtR8UW6WfJQTjY5VuHvHcwxshF5l+u7AqF+rgQEpXxQkKV8Jj9Biu6Vl1lHJaNjlpKobNFU4zkfA6RbmhMKblpdoulaK66YAm0hMeIibb6PeM6LdTQdyv9E2RJWq6MXNsyY2VnKyxTxuWmJpGJQ8ZKSl2UZtsxiBxeyaUY9+FI694wBXPdZ0AQh1EYx4xP46kXRHBnZ1gzsbIPaw3Vu/LpN2mm4XHBory+MR52yH2qZ8LUkCnGmRBjKaR67k0AcR8Coy+1kr+g1jKiAWHUtCi5mnPgdrw1AJSG7dGnQntWJshAzkCrneliB5CAFENsfCHPL+qbylt7n6Q1T+HAisxGSLKfu4JoCpZjT6a4xRT71MFUmFXvebY2xSQvfoWHb6VsFqm1tHygUjmbroyR9y944ww08QFN7LktmmEHzPASLEmLpY/wcFiiW2LptVAAN3nOVqXSqUzknIlJpdb2qtuEA9tMP9XKP/Iu7/y8NjeGPtkRz5Wq7SdovbMJnq20NFK17oOUiwb63LzT4M3TyJWK4HT0aKYS0Kd2a9uBCgTT2bq5/mvc4eEIU4xG4PsesCDqCO1z+ufKpBHugZq8FeorRf6VmfdBfiQ9XR45HXbi9kf4zRJ3i2Uw7AOFd0ssw//9JXgseupponu3Ht8MwYEDL+KZ0cDehyeBBqd3BB31B2pn+SIzY0D/KYqYsKMO3LK36N88hc7+1cgZhHHU9dVIoxCm8SW+GukINQm67bSEaEde2otnDX901nAfRGaiPTYSutYJ5uiRdthRfldeot1IlPfI8i9Abcb8AeLYz3+diJ//Xgm1hIg6IHpdEL1LQCxvEwcK0Q4I3IP8+8ZQX/9SuwGoJGxud+STFtSQtpli3/trpqZa3VwX54rq/p9M/gA=&lt;/diagram&gt;&lt;/mxfile&gt;"><defs/><g transform="translate(0.5,0.5)"><rect x="288" y="0.75" width="75" height="202.5" rx="11.25" ry="11.25" fill="#e1d5e7" stroke="#9673a6" pointer-events="none"/><path d="M 243 72 L 273 102 L 243 132 L 213 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(231.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">256<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 168 72 L 198 102 L 168 132 L 138 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(156.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">512<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 198 102 L 208.22 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 212.16 102 L 206.91 104.63 L 208.22 102 L 206.91 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 120.75 102 L 135.75 102 L 123 102 L 133.22 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 137.16 102 L 131.91 104.63 L 133.22 102 L 131.91 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 273 102 L 288 102 L 273 102 L 283.22 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 287.16 102 L 281.91 104.63 L 283.22 102 L 281.91 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 482.25 72 L 512.25 102 L 482.25 132 L 452.25 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(470.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">512<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 407.25 72 L 437.25 102 L 407.25 132 L 377.25 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(395.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">256<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 437.25 102 L 447.47 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 451.41 102 L 446.16 104.63 L 447.47 102 L 446.16 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 360 102 L 375 102 L 362.25 102 L 372.47 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 376.41 102 L 371.16 104.63 L 372.47 102 L 371.16 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><rect x="526.5" y="42" width="120" height="120" rx="18" ry="18" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><path d="M 512.25 102 L 521.72 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 525.66 102 L 520.41 104.63 L 521.72 102 L 520.41 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><rect x="0.75" y="42" width="120" height="120" rx="18" ry="18" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/>
 <image xlink:href="img/static/x0.gif" x="8.25" y="49.5" width="105" height="105" fill="#f5f5f5" stroke="#666666" pointer-events="none"/>
 <image xlink:href="img/static/x1.gif" x="534" y="49.5" width="105" height="105" fill="#f5f5f5" stroke="#666666" pointer-events="none"/>
 <image xlink:href="img/static/y.gif" x="293.25" y="6.75" width="64.5" height="190.5" fill="#f5f5f5" stroke="#666666" pointer-events="none"/></g></svg>
 #+end_html
#+end_center

#+begin_center
+ 鍵となるアイディア: *これらのカテゴリ変数は /直接/*

  *命題論理的モデルとして扱える*

  特に, *カテゴリ数 = 2 → 活性化値が命題変数の値に相当 (0/1 = 真/偽)*
#+end_center

** State Autoencoder (*/訓練前/*)

 [[png:sae/state-ae-before]]

** State Autoencoder (_/訓練後/_)

 [[png:sae/state-ae]]

** Gumbel-Softmax: Differential Approximation of Gumbel-Max        :noexport:

#+begin_larger
It uses *annealing* to approximate discrete vectors
#+end_larger

#+begin_smaller
Gumbel-Max: Method for drawing one-hot vector sample ($z$) from category probability ($x$)

+ E.g.: $x=[0.1, 0.1, 0.8] \rightarrow z = [1,0,0] \text{or} [0,1,0] \text{or} [0,0,1]$

+ $z = \text{ GumbelMax}(x) = [ i == \arg \max_j (\text{ Gumbel}(0,1)+\log x_j) \; ? \; 1 : 0 ]$

+ argmax is non-differentiable → softmax approximation (differentiable)

+ $z = \text{ GumbelSoftmax}_\tau (x) = \text{ Softmax}( [\text{ Gumbel}(0,1)+\log x_j]/\tau )$

+ Temparature $\tau \rightarrow 0$ , $z\rightarrow \text{one-hot vector}$

  # \[
  # \text{ GumbelSoftmax}_\tau (x) \rightarrow \text{ GumbelMax}(x) \quad (\tau\rightarrow 0)
  # \]
#+end_smaller

#+begin_container-fluid
#+begin_row-fluid
#+begin_span2

#+end_span2
#+begin_span8
#+begin_center
[[png:sae/gumbel]]
#+end_center
#+end_span8
#+begin_span2

#+end_span2
#+end_row-fluid
#+end_container-fluid

#+begin_note
Maddison et. al., 2014
#+end_note

* 実験: State Autoencoder (SAE) の作った命題変数で「論理的に正しく推論できるか?」

全ての状態遷移を生成して、それぞれをPDDLアクションに翻訳、汎用ソルバを用いて解く

(つまり、アクションモデル獲得は省略)

#+begin_src lisp
 0011 → 0101  

　　　↓       ;; 状態遷移ごとにアクションを１つ作製
(:action       action-0011-0101
               ;; 前提条件は遷移前の状態をそのまま使う
 :precondition (and (b0-false) (b1-false) (b2-true) (b3-true))
               ;; 効果 = 状態の差分
 :effect       (and (not (b1-false)) (b1-true)
                    (not (b2-true))  (b2-false)))
#+end_src

それぞれのビットを命題に変換

#+begin_center
 $i$-th bit が 1 → 命題 ($b_i$ -true)

 $i$-th bit が 0 → 命題 ($b_i$ -false)
#+end_center

*** Example PDDL

# Examples in $N=25$ (in the paper we bypassed PDDL-SAS translater)

#+begin_src lisp
(define (domain latent)
 (:requirements :strips)
 (:predicates (b0-true) (b0-false) (b1-true) ... (b24-false))

 (:action a10000010010110111100011111000010001011111110011111
  :parameters () :precondition
  (and (b0-true) (b1-false) (b2-false) ... (b24-true))
  :effect (and (not (b5-false))  (b5-true)
               (not (b6-true))   (b6-false)
               (not (b13-false)) (b13-true)
               (not (b20-false)) (b20-true)))

 (:action a10000010010110111100011110000001001011011110001110
  ...
#+end_src

** Step 3: このPDDLを使って問題を解き

  [[png:overview/planning4]]

** Step 4: プランを実行し

  [[png:overview/planning5]]

** Step 5: 結果を可視化

  [[png:overview/planning6]]

# * AMA_1 Experiments
# 
# SAE: *trained with 20k images* (note: > 360k entire states in 8-puzzle)
# 
# + */SAE is generalizing/*
# 
# AMA_1: requires the entire transistions (note: > 1M transitions in 8-puzzle)
# 
# + _/AMA_1 is NOT generalizing, being an oracle/_
# 
# Planner: a State-of-the-Art, Fast Downward (Helmert, 08)
# 
# + $A^*$ *(optimal search)* : *It must find an optimal solution*
# 
# + Runtime: ~3sec (instances are too small for symbolic systems)

** 8-puzzle Results with MNIST tiles (MNIST 8-puzzle)

 [[png:results/mnist-plan]]

 #+begin_larger
 #+begin_xlarge
 #+begin_alignright
  → *31手の最適解を返却*
 #+end_alignright
 #+end_xlarge
 #+end_larger

** 写真ベースの8パズル: タイルが明確に分離されていない (Mandrill 8パズル)

MNIST 8パズルは タイル同士がくっきり分離していた -> 境界がよく分からなくても大丈夫か?

[[png:results/mandrill-intro]]

** 写真ベースの8パズル: タイルが明確に分離されていない (Mandrill 8パズル)

[[png:results/mandrill-plan]]

#+begin_xlarge
#+begin_alignright
 → *最適解*
#+end_alignright
#+end_xlarge

** 写真ベースの8パズル: タイルが明確に分離されていない (Spider 8パズル)

[[png:results/spider-plan-new]]

#+begin_center
#+begin_larger
*Latplan は この問題が8パズルと呼ばれていることすら知らないことに注意*; MNIST, Mandrill, Spider は 機械にとっては全て異なるドメイン
#+end_larger
#+end_center

** ハノイの塔 (3 disks, 4 disks)

全く別種のパズルでも同じシステムで解くことが出来る

[[png:results/hanoi3]]

[[png:results/hanoi4]]

#+begin_alignright
#+begin_xlarge
 → *最適解* (7 steps,15 steps)
#+end_xlarge
#+end_alignright

** ライツアウト

全く別種のパズルでも同じシステムで解くことが出来る

[[png:results/lightsout_new4x4]]

#+begin_alignright
#+begin_xlarge
 → *最適解*
#+end_xlarge
#+end_alignright

** Twisted Lights Out

入力画像に歪みを加えて「格子状」という制限をなくす

[[png:results/lightsout_twisted_new4x4]]

#+begin_alignright
#+begin_xlarge
 → *最適解*
#+end_xlarge
#+end_alignright

** ノイズ耐性

SAE の実装に Denoising AE を用いているため、ノイズに惑わされず命題にマップできる

[[png:results/noise-new]]

#+begin_larger
#+begin_alignright
 → *既存のディープラーニング技術をすぐ活用できる*

 → *速い訓練、頑健性、精度*
#+end_alignright
#+end_larger
* Why bother the off-the-shelf planner? Shouldn't the blind search do? :noexport:

*Domain-independent lowerbounds works, /SURPRISINGLY!/*

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
+ This is *NOT* a trivial finding!

  lower-bounds are...

  + taylored for *man-made* domains
  
  + assumes the domain has a *structure*

  Blind search even sometimes outperform sophisticated methods on man-made instances (Edelkamp 12)

+ lb works → *the more difficult problems can be solved (future work)*
#+end_span6
#+begin_span6
#+begin_smaller

| domain          | Dijkstra   | A*+PDB          | instances |
|-----------------+------------+-----------------+-----------|
| MNIST           | 210k(50k)  | *97k* (53k)     |        13 |
| Mandrill        | 176k(13k)  | *112k* (57k)    |        12 |
| Spider          | 275k(65k)  | *58k* (30k)     |        13 |
| Hanoi (4 disk)  | 20.7(29.7) | *12.6* (22.0)   |        30 |
| LightsOut (4x4) | 433(610.4) | *130.3* (164.4) |        16 |
| Twisted   (4x4) | 398(683.1) | *29.3* (62.4)   |        16 |
|-----------------+------------+-----------------+-----------|

#+end_smaller
#+end_span6
#+end_row-fluid
#+end_container-fluid

* Why Gumbel-Softmax is necessary?                                 :noexport:

+ Alternative 1: Use a normal NN and *round* the encoder output?
  + ✘ The *decoder* is not trained with 0/1 value
+ Alternative 2: Include the *round* operation in a NN?
  + ✘ Rounding is *non-differentiable* / *Backpropagation impossible*

* State Autoencoder 実験まとめ

+ SAE は小さな入力から学習できる ::

     訓練画像 20k枚 → 見たことのない 360k 枚の画像を正しく復元できる

+ SAEの生成した命題変数は健全である ::

     それを用いて記号的プランナが推論を行える

+ Latplan は核となる記号的プランナの理論的性質を受け継ぐ ::

  最適コスト解を保証するアルゴリズム (A*等) を使えば、画像による結果も最適解

  完全性のあるアルゴリズムを使えば、解が存在する場合には必ず発見する

* *** ☕ Break ☕ ***

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

*/☕ Break ☕/*

#+begin_xlarge
*AMA_2 Overview*

*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*/AMA_2 Overview/*

*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* SAE Feasible! Now what?

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
*先程の実験ではアクションモデル獲得(AMA)をスキップ、外部からシミュレータを与えた*

SAEは *現実の状態を命題表現にするだけ* なので、 *世界がどう変化するかは知らない*

# #+begin_center
# + *AMA は、少数の状態遷移の例から世界のシミュレータを作る*
# #+end_center
#+end_span6
#+begin_span6
 [[png:overview/ama1]]
#+end_span6
#+end_row-fluid
#+end_container-fluid

#+begin_larger
#+begin_alignright
+ → 新たなニューラルネット *AMA_2* でAMAを実行
#+end_alignright
#+end_larger

* AMA_2 : */本物の/* AMA method

*入力:* 状態遷移 $\{ (s,t) \ldots \}$

#+begin_alignright
(SAEで命題論理表現に変換済み)
#+end_alignright

+ *アクションシンボル・グラウンディング*
  
  #+begin_src lisp
  (:action slide-up-tile7-at-x0-y1 ...
  #+end_src
+ *前提条件の学習*
  
  #+begin_src lisp
   :precondition (and (empty x0 y0) ...)
  #+end_src
+ *効果の学習*
  
  #+begin_src lisp
   :effects      (and (empty x0 y1) (not ...)))
  #+end_src

** Action Symbol Grounding                                         :noexport:

Identifing a *type* of transition (clustering)

#+begin_container-fluid
#+begin_row-fluid
#+begin_span9
#+begin_src lisp
(:action slide-up-tile7-at-x0-y1 ...
 :precondition ...
 :effects      ...)
 #+end_src
 #+end_span9
#+begin_span3
initial state
[[png:8puzzle-standard-tile7]]
#+end_span3
#+end_row-fluid
#+end_container-fluid

# + E.g. *slide-up-tile7-at-x0-y1* action abstracts all transitions that looks like this
# 
# + → not merely an individual transition

** Action Precondition Learning                                    :noexport:

Learns *when* that transition is allowed

#+begin_container-fluid
#+begin_row-fluid
#+begin_span9
#+begin_src lisp
(:action slide-up-tile7-at-x0-y1 ...
 :precondition (and (empty x0 y0) ...)
 :effects      ...)
#+end_src
#+end_span9
#+begin_span3
initial state
[[png:8puzzle-standard-tile7]]
#+end_span3
#+end_row-fluid
#+end_container-fluid

** Action Effect Learning                                          :noexport:

Learns *what* happens after the transition

#+begin_container-fluid
#+begin_row-fluid
#+begin_span9
#+begin_src lisp
(:action slide-up-tile7-at-x0-y1 ...
 :precondition (and (empty x0 y0) ...)
 :effects      (and (empty x0 y1) (not ...)))
#+end_src
#+end_span9
#+begin_span3
initial state
[[png:8puzzle-standard-tile7]]
#+end_span3
#+end_row-fluid
#+end_container-fluid

#+begin_alignright
+ AMA_2 perform all three processes.
#+end_alignright

** なぜ */アクションシンボル/* が必要なのか?

 前提条件と効果の学習の必要性は自明、しかしなぜアクションシンボル?

 プランナは グラフ上の *前方探索* *(Dijkstra, $A^＊$ )* で問題を解くのが主流

 アクションシンボルなしでは、 *後者状態の生成* が難しくなる

 + $|z|=36 \text{bit}$ のSAEの場合:

   → 可能性のある後者状態は *$2^{36}$* 個あり、これを二値分類機などでふるいわけ

 + */アクションシンボルがあれば/ (例: 上下左右)*

 + → 考えるべき後者状態の候補は */定数時間/ で列挙できる* (例: 4つ).

** 課題: 条件/効果の学習が自明でない

*Training 入力 には、その遷移が何のアクションかを表すラベルがついていない*

ただの画像ペア

[[png:ama/action-symbol]]

+ → *どの状態遷移が「同じタイプの遷移」=「同じアクション」なのかを知らない*

** 課題: 条件/効果の学習が自明でない

#+begin_xlarge
Case 1: 線形な探索空間
#+end_xlarge

+ *線形な空間ではアクションラベルは必要ない*; どうせ一直線、分岐なし
+ この場合 AMA ≡ *予測タスク* ($\approx$ ビデオの次フレームの予測)
  + 状態遷移関数 $a(s) = t\;$ を近似するニューラルネットを、誤差 $|t-a(s)|$ を最小化して訓練すればいい。
  + 現状のディープラーニング界のAMAは全てこれ

[[png:ama/linear]]

** 課題: 条件/効果の学習が自明でない

#+begin_xlarge
Case 2: 実際は */グラフ/*
#+end_xlarge

#+begin_container-fluid
#+begin_row-fluid
#+begin_span7
#+begin_center
+ *複数のアクションシンボル*
+ *ノードによって /子供の数が異なる/*

+ *アクション $a$ ごとに $a(s) = t\;$ を訓練することはできない* 、
  なぜなら *与えられた (s,t) がどの $a$ に属するのか未知.*
#+end_center

#+end_span7
#+begin_span5
[[png:ama/non-linear]]
#+end_span5
#+end_row-fluid
#+end_container-fluid

#+begin_alignright
#+begin_larger
+ *ここで問われるべき正しい質問*: AMAのために *学習するべき関数は何なのか?*
#+end_larger
#+end_alignright

* AMA_2 Overview

[[png:ama/overview0]]

** AMA_2 Overview

 [[png:ama/overview1]]

** AMA_2 Overview

 [[png:ama/overview1-1]]

** AMA_2 Overview

 [[png:ama/overview2]]

** AMA_2 Overview

 [[png:ama/overview3]]
* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*AMA_2 Overview*
 
*/Action AutoEncoder (AAE)/*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* Action AutoEncoder

 AMAのために *学習するべき関数は何なのか?*

#+begin_center
#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
+ $a(s) = t$ ? ダメ!
+ $a$ は実際には *変数である*
#+end_span6
#+begin_span6
+ $apply(a,s) = t$
+ 状態遷移とは、写像 $a \rightarrow t$ を

  *$s$ で条件付けたもの* である
  
+ *学習するべき関数はこれだ!*
#+end_span6
#+end_row-fluid
#+end_container-fluid
#+end_center
* Action AutoEncoder

[[png:aae/aae-0]]

** Action AutoEncoder

[[png:aae/aae-1]]

** Action AutoEncoder

[[png:aae/aae-2]]

** Action AutoEncoder

[[png:aae/aae-3]]

* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*AMA_2 Overview*

*Action AutoEncoder (AAE)*

*/Action Discriminator (AD)/*
#+end_xlarge
#+end_center

* AAE はアクションシンボルと効果だけ

前提条件は学習しない

 [[png:ama/overview1-1]]

* Action Discriminator は 二値分類機

*二値分類機*, どの遷移が正しいか答える

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[png:aae/ad]]
#+end_span6
#+begin_span6
+ *正例* と *負例* から訓練

#+begin_center
+ *正例: 正しい状態遷移 (観察されたデータ) は手元にある.*
+ *負例 ( /許されない遷移/ ) のデータはどこにある...?*
#+end_center
#+end_span6
#+end_row-fluid
#+end_container-fluid

** 許されない遷移

+ */許されない遷移/*: *許される遷移ではない全ての遷移.*
  + そもそも「許される遷移」を知りたいので未知、従って *機械的生成は不可能.*

    生成が不可能ならデータを集めれば良い?

#+begin_container-fluid
#+begin_row-fluid
#+begin_span2

#+end_span2
#+begin_span6
[[png:aae/teleportation]]
#+end_span6
#+begin_span2

#+end_span2
#+end_row-fluid
#+end_container-fluid

+ *瞬間移動は物理法則に反する*. (少なくとも巨視的には)
  + 自律行動AIにとって

    *許されない遷移は /起こり得ない/, 従って /観察され得ない/.*

#+begin_alignright
#+begin_larger
+ 許されない遷移 のデータは *生成も観察も出来ない!*
#+end_larger
#+end_alignright

** PU-Learning framework (Elkan & Noto, KDD 08')

*正例(positive) & 未分類例(unlabelled)から 正例/負例 識別機を学習できる!*

+ 正例: Training 入力. *観察されたデータは全て正しい、保証済み*
+ 未分類例: *AAE が生成する全ての子ノード候補*
  + どれかは正例、どれかは負例

#+begin_alignright
+ PU学習で AD を訓練可能!
#+end_alignright

** 後者関数

AAE で *子ノード候補を列挙*; AD/SDで *負例をフィルタリング*.

\begin{align*}
  Succ(s) &= \{t = apply(a,s) \; | \; a \in \{0\ldots 127\},\\
          & \qquad \land AD(s,t) \geq 0.5 \\
          & \qquad \land SD(t) \geq 0.5 \}
\end{align*}

# & \qquad \land SD(t) \geq 0.5 \\
# & \qquad \land Encode(Decode(s)) \equiv s \\
# & \qquad \land Apply(Action(t,s),s) \equiv t \}

後者状態を生成する関数が手に入ったため、
古典的な記号的前方探索アルゴリズム (A*, Dijkstra etc.) が使用可能

* Planning using AMA_2                                             :noexport:

Using a symbolic forward-search planner

+ Effects/preconditions are embedded in NNs, but there are *discrete action labels*
+ Simple $A^*$ with *goal-count* heuristics
  + The number of different bits from the goal

* AMA_2 Experiments 1

AAE と AD を使ってプランニングを行うことは可能?

+ それぞれのドメインごとに *100問* 問題を生成
  + 初期状態はゴールから 自己回避ランダムウォークで生成
  + (benchmark A) 7歩
  + (benchmark B) 14歩
+ 180 sec. time limit

# The failures are due to timeouts
# (the successor function requires many calls to the feedforward neural nets,
#  resulting in a very slow node generation).

# We next examine the accuracy of the AD and SD (\reftbl{tab:aae-results}).
# We measured the type-1/2 errors for the valid and invalid transitions (for AD) and states (SD).
# Low errors show that our networks successfully learned the action models.

** Results

Noise are applied to the planning inputs (init/goal images)

G: Gaussian noise, s/p: salt/pepper noise

+ *Easy instances: Majority of instances are solved*
+ *Harder instances: Still many instances are solved*


|   | /          |   < |     |   > |   < |    |   > |
|   | step       |   7 |   7 |   7 |  14 | 14 |  14 |
|   | noise      | std |   G | s/p | std |  G | s/p |
|---+------------+-----+-----+-----+-----+----+-----|
|   | MNIST      |  72 |  64 |  64 |   6 |  4 |   3 |
|   | Mandrill   | 100 | 100 | 100 |   9 | 14 |  14 |
|   | Spider     |  94 |  99 |  98 |  29 | 36 |  38 |
|   | LightsOut  | 100 |  99 | 100 |  59 | 60 |  51 |
|   | Twisted LO |  96 |  65 |  98 |  75 | 68 |  72 |
| / | Hanoi      |  37 |  44 |  39 |  15 | 18 |  17 |

* AMA_2 Experiments 2                                              :noexport:

How accurate are Action Discriminators and State Discriminators?


#+begin_container-fluid
#+begin_row-fluid
#+begin_span7
Measure the type-1 / type-2 error in %

#+begin_smaller
|          |    SD |    SD |   |    AD |    AD |   AD |   AD |
|          | type1 | type2 |   | type1 | type2 | 2/SD |  2/V |
|----------+-------+-------+---+-------+-------+------+------|
| MNIST    |  0.09 | <0.01 |   |  1.55 |  14.9 | 6.15 | 6.20 |
| Mandrill | <0.01 | <0.01 |   |  1.10 |  16.6 | 2.93 | 2.94 |
| Spider   | <0.01 | <0.01 |   |  1.22 |  17.7 | 4.97 | 4.91 |
| L. Out   | <0.01 |   N/A |   |  0.03 |  1.64 | 1.64 | 1.64 |
| Twisted  | <0.01 |   N/A |   |  0.02 |  1.82 | 1.82 | 1.82 |
| Hanoi    |  0.03 | <0.01 |   |  0.25 |  3.50 | 3.79 | 4.07 |
#+end_smaller

#+end_span7
#+begin_span5
#+begin_smaller
+ (SD type-1) :: Generate all valid states and count the states misclassified as invalid.
+ (SD type-2) :: Generate reconstructable states, remove the valid states (w/ validator),
                 sample 30k states, and count the states misclassified as valid.
                 N/A means all reconstructable states were valid.
+ (AD type-1) :: Generate all valid transitions and count the number of misclassification.
+ (AD type-2) :: For 1000 randomly selected valid states, generate all successors,
                 remove the valid transitions (w/ validator), then count the transitions misclassified as valid.
+ (2/SD, 2/V) :: Same as Type-2, but ignore the transitions whose successors are
                 invalid according to SD or the validator.
#+end_smaller
#+end_span5
#+end_row-fluid
#+end_container-fluid

* AMA_2 Experiments 2                                              :noexport:

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
Action Discriminators の精度は?

Measure the type-1 / type-2 error in %

|   |          | type1 | type2 |
|---+----------+-------+-------|
|   | MNIST    |  1.55 |  6.15 |
|   | Mandrill |  1.10 |  2.93 |
|   | Spider   |  1.22 |  4.97 |
|   | L. Out   |  0.03 |  1.64 |
|   | Twisted  |  0.02 |  1.82 |
| / | Hanoi    |  0.25 |  3.79 |

#+end_span6
#+begin_span6
#+begin_smaller
+ (AD type-1) :: 全ての正しい状態遷移を生成し、ADが負例とした率
+ (AD type-2) :: ランダムに状態遷移を生成し、正しいものを取り除き、ADが正例とした率
#+end_smaller
#+begin_larger
+ *あるていど正確.*
#+end_larger
#+end_span6
#+end_row-fluid
#+end_container-fluid

# #+begin_larger
# #+begin_alignright
# + *Reasonably accurate.*
# #+end_alignright
# #+end_larger

* まとめ

+ *Latplan Architecture* 

  　　画像で示された現実世界の問題を *自力で記号モデルに変換し* 、記号的探索によって解く
  # + 入力 : Unlabelled pairs of images, initial image, goal image
  # + Output : Visualized plans to achieve the goal
+ *State AutoEncoder(SAE)* : */現在状態の生データ/ ↔ /命題表現/*
+ *Action AutoEncoder(AAE)* : */状態遷移/ ↔ /アクションシンボル/, /効果/*
+ *Action Discriminator(AD)* : */状態遷移/ → /真偽値/* , *PU学習で訓練*

#+begin_center
+ *古典プランニングで必要なシンボルグラウンディングのうち、２つを解決した!*
  | シンボルの種類       |                              |
  |----------------------+------------------------------|
  | 命題変数シンボル     | *Solved!*                    |
  | アクションシンボル   | *Solved!*                    |
  | オブジェクトシンボル | R-CNN (Computer Vision) etc? |
  | 述語シンボル         | SCAN???                      |

  速度や精度が今後の課題
#+end_center

** Future Work (問題ドメイン)

LatPlan は あくまで *アーキテクチャ*

*SAE の実装を変えれば(原理的には)画像以外の任意の入力データに対応可能*

+ テキスト用のAE [Li et.al. 2015]、 音声用のAE [Deng, Li, et al. 2010]
+ 改造してSAEにすれば・・・
  + *Here's an Apple, Here's a pen → oh, ApplePen!*
+ SAEを学習してプランナで解く

  → *数千ステップの /論理的な/ 言語レベル推論 (反射ではなく) を可能に*
  
  + これは、本質的に近視眼的/貪欲なエージェント(*強化学習など*)には不可能

#+BEGIN_NOTE
 "A hierarchical neural autoencoder for paragraphs and documents." (2015)

 "Binary coding of speech spectrograms using a deep auto-encoder." (2010)
#+END_NOTE

# ** Future Work (Extended planning formalism)
# 
# + Latplan assumes nothing about the environment machinery (grids, movable tiles...)
# + Latplan assumes *fully-observable*, *deterministic* domains
# + Next step: *Extending Latplan to MDP, POMDP*
#   + Gumbel-Softmax layer → just a Softmax layer? (probability)
#   + $AO^*$, $LAO^*$ algorithms for *optimal planning under uncertainty*
#     
# ** Other Future Works
# 
# Extracting rules from AAE/AD (discussed later)
# 
# Extracting much higher-order rule (predicates, HTN)

* *neural-symbolic* AI !

[[png:latplanlogo]]

* Appendix

Using the Remaining Time (Discussion)

** Konidaris et. al (2014, 2015): 

Structured input (e.g. *light switch*, *x/y-distance*) ↔ unstructured image

Action Symbols (*move, interact*)

Just converting Semi-MDP /model/ to a PDDL /model/.

Could be used for extracting PDDL from AAE/AD

** NNs for solving combinatorial tasks:

TSP (Hopfield and Tank 1985)

Neurosolver for ToH (Bieszczad and Kuchar 2015)

*The input/output are symbolic.*

** Other Work Combining Symbolic Search and NNs

Embedded NNs *inside* a search to provide the *search control knowledge*

(i.e. node evaluation function)

Sliding-tile puzzle and Rubik’s Cube (Arfaee et al. 2011)

Classical planning (Satzger and Kramer 2013)

The game of Go (AlphaGo, Silver et al. 2016)

** Deep Reinforcement Learning (DRL) (Mnih et al. 2015, DQN)

DQN assumes *predetermined action symbols* (↑↓←→+ buttons).

DQN *relies on simulators*. ↔ Latplan *reverse-engineers a simulator*.

*DQN does not work* when it does not know *what action is even possible!*

** Other Interesting Systems

SCAN system (deepmind)

+ Maps *continuous* latent ↔ *human-provided* symbolic vector

δ-ILP (Inductive Logic Programming)

+ ILP robust to noise
  + Extracting rules from AAE/AD to form a PDDL?

** Why not individual pixels? Why DL?

*Systems based on individual pixels lack generalization*

+ Noise / variations can make the data entirely different

  [[png:results/noise]]

+ must acquire the *generalized features*

+ = a nonlinear function that recognize the entanglements between multiple pixels

** Learning vs Planning
  
 Main differences: Purposes and the abstraction layer

 #+begin_container-fluid
 #+begin_row-fluid
 #+begin_span6
 *Machine Learning, Neural Networks* 
 
 for *Recognition, Reflex*
 + *Subsymbolic 入力* (continuous)
   
   Images, Audio, unstructured text: 
 + *Soft Intelligence*:
   
   　 */Reflex Agent/, /Immediate/ actions*
   #+begin_smaller
   *Pavlov's dog* : bell → drool

   *Autonomous Driving* : Pedestrian → Stop.

   *Machine Translation* : Sentence → Sentence

   *Eval. Function for Go* : board → win-rate
   #+end_smaller
   #+begin_larger
   ☺ Efficient 1-to-1 mapping
   
   ☹ Simple tasks
   #+end_larger
 #+end_span6
 #+begin_span6
 *Deliberation, Search*

 for *Planning, Game, Theorem Proving*
 + *Symbolic 入力/Output*
   
   Logic, objects, induction rules
 + *Hard Intelligence by Logic:*

   　 */Multi-step/ strategies*
   
   #+begin_smaller
   *Rescue Robot* : actions → help the surviver

   *Theorem Proving* : theorems → QED

   *Compiler* : x86 instructions
   
   *Game of Go* : stones → Win
   #+end_smaller
   #+begin_larger
   ☺ Ordering constraint + complex tasks
   #+end_larger
 #+end_span6
 #+end_row-fluid
 #+end_container-fluid

+ AlphaGo = Subsymbolic (DLNN eval. function) + Symbolic (MCTS)

** Human-Competitive Systems

 AlphaGo = Subsymbolic (NN eval. func) + Symbolic (MCTS)
 + However, *domain-specific* -- specialized in Go, "Grids" / "Stones" are known
 + *Huge expert trace DB* --- Not applicable when data are scarse (e.g. *space exploration*)
 + */Is supervised learning necessary for human?/*
  
   *True intelligence should search / collect data by itself*

 DQN = Subsymbolic (DLNN) + Reinforcement Learning (DLNN)

Domain-independent Atari Game solver (Invader, Packman…), however:
 + RL Acting: Greedily follow the learned policy → *no deliberation!*
 + You can survive most Atari games *by reflex*
  
 # 実際 *Sokoban など論理思考ゲームでは性能が悪い* ↔ 倉庫番ソルバ

** Latplan Advantages

#+begin_xlarge
*/Perception/* based on DLNN
#+end_xlarge

--- Robust systems augmented by the latest DL tech

#+begin_xlarge
*/Decision Making/* based on Classical Planning
#+end_xlarge

--- *Better Theoretical Guarantee than Reinforcement Learning*

#+begin_center
*Completeness* (Finds solution whenever possible), */Solution Optimality/*
#+end_center

--- *Decision Making Independent from Learning*

#+begin_center
*/Unsupervised/* (No data required), *Explainable* (Search by logic)
#+end_center

# 今まではNNとの相性から強化学習が優勢だったが *もうその必要はない*

*** When Latplan returns a /wrong/ solution?

 *Machine learning may contain errors* (convergence /only on/ $t\rightarrow \infty$, not on real time)

 + Images → Fraud symbols/model/graph

 + *Optimal path on a fraud graph* or *graph disconnected*
  
   A* completeness, soundness, optimality (admissible heuristics) 

 + Fraud visualized plan (noisy) / no plan found

 #+begin_center
 #+begin_larger
 LatPlan may make */wrong observations/* but no */wrong decisions/*
 #+end_larger

 BTW, "correctness" is defined by error prone observations by humans anyways ...
 #+end_center

 #+begin_alignright
  (completeness, optimality) → better reliablility than Reinforcement Learning
 #+end_alignright

*** Reinforcement Learning

Not only *perception* but *decision making also depends on training*

+ */Each training result does not have admissibility/*

+ */When the learned policy is wrong, the solution could be suboptimal/*

#+begin_quote
... AlphaGo was unprepared for Lee Sedol’s Move 78 because it
didn’t think that a human would ever play it.

#+begin_alignright
Cade Metz. "In Two Moves, that Redifined the Future." /Wired/, 2016
#+end_alignright
#+end_quote

#+begin_center
#+begin_larger
RL may make *wrong decisions*.
#+end_larger
#+end_center
** Future Work (SAE)

SAE can generate propositional symbols (state $s = \{q,r\ldots\}$)

+ 1st-order logic (predicate $p(a,b)$ )

+ We need *object recognition from images* (parameters $a,b$)

+ SAE with networks for object recognition (e.g. R-CNN) should achieve this

** Why symbols?

Symbols are strong abstraction mechanisms becasue

+ Meanings do not matter ::
     You do not have to understand it: Does a symbol $X$ mean an apple or a car?
 
     Logical reasoning can be performed by mechanical application of rules
     
     + Domain-independent planning : *mystery* vs *nomystery*

       Logistic domains where *symbol names are mangled* (truck → shark)

+ Composable :: 
     A latent vector is a conjunction (and)
     
     Heuristic functions use modus ponens to derive guidance
